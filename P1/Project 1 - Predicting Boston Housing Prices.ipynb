{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation & Validation Project\n",
    "\n",
    "## <font color=\"#347797\">Project Description</font>\n",
    "\n",
    "You want to be the best real estate agent out there. In order to compete with other agents in your area, you decide to use machine learning. You are going to use various statistical analysis tools to build the best model to predict the value of a given house. Your task is to find the best price your client can sell their house at. The best guess from a model is one that best generalizes the data.\n",
    "\n",
    "For this assignment your client has a house with the following feature set: [11.95, 0.00, 18.100, 0, 0.6590, 5.6090, 90.00, 1.385, 24, 680.0, 20.20, 332.09, 12.13]. To get started, use the example scikit implementation. You will have to modify the code slightly to get the file up and running.\n",
    "\n",
    "When you are done implementing the code please answer the following questions in a report with the appropriate sections provided."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from boston_housing_students import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "city_data = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(city_data.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <font color=\"#347797\">Questions and Report Structure</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Statistical Analysis and Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of data points              : 506\n",
      "Number of features                 : 13\n",
      "Minimum house price                : 5,000.00\n",
      "Maximum house price                : 50,000.00\n",
      "Mean house price                   : 22,532.81\n",
      "Median house price                 : 21,200.00\n",
      "Standard deviation of house prices : 9,188.01\n"
     ]
    }
   ],
   "source": [
    "explore_city_data(city_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Evaluating Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Which measure of model performance is best to use for regression and predicting Boston housing data? ***\n",
    "\n",
    "Mean Absolute Error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why is this measurement most appropriate? Why might the other measurements not be appropriate here?***\n",
    "\n",
    "We have seen a few different measurement for performance for both classification and regression. Since this is  a regression problem, Precision, Recall, Accuracy and F1 score are useless. As for regression we have seen the mean squared error (MSE) and the mean absolute error (MAE). I would argue that using the MAE is preferable because we do not necessarily want to put more weight on outliers. Using MSE will square the error and that will be heavy. Some houses will be very expensive and they should not play a major role in predicting the house prices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why is it important to split the data into training and testing data? What happens if you do not do this?***\n",
    "\n",
    "Testing with the same, or part of the same, dataset that was used for training will introduce bias in the predictions. The model will make predictions on the same data points that it used for training, thus resulting in cheating. If we were to chose a model based on those predictions, the results could be catastrophic. We would have no idea whatsoever what the performances would be on examples that the model never saw before."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Which cross validation technique do you think is most appropriate and why?***\n",
    "\n",
    "At this point in the class we have only seen K-Fold, so I'm gonna go with that. As far as I can tell, all the other CV techniques are variations on K-Fold. The main advantage of K-Fold is that it allows to train on everthing and test on everything. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What does grid search do and why might you want to use it?***\n",
    "\n",
    "Hyperparameters can only be learned empirically. It's something that we have to see if it works well or not. Doing it manually is tedious and fortunatly librairies like Scikit Learn makes it super easy to do. It will do all the possible permutations of the specified hyperparameters and run the learning algorithms with them. It will select the best ones and use them for further predictions. It's all nicely wrapped and easy to use. So basically grid search saves a lot of hassels and programming time, but takes more time to train. So if the dataset is too big or you have a learning algorithm that naturally takes a lot of time to train, like neural nets, then you might want to think about it twice or at least do a smart selection first and not just throw a range of hyperparameters to try at random."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### 3) Analyzing Model Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Look at all learning curve graphs provided. What is the general trend of training and testing error as training size increases?***\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"figure_1.png\" alt=\"Depth 1\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"figure_2.png\" alt=\"Depth 2\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"figure_3.png\" alt=\"Depth 3\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"figure_4.png\" alt=\"Depth 4\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"figure_5.png\" alt=\"Depth 5\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"figure_6.png\" alt=\"Depth 6\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"figure_7.png\" alt=\"Depth 7\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"figure_8.png\" alt=\"Depth 8\">\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src=\"figure_9.png\" alt=\"Depth 9\">\n",
    "        </td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src=\"figure_10.png\" alt=\"Depth 10\">\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and testing error are quite high with a small depth for the decision tree. The testing error soon stabilizes no matter the depth of the depth of the decision tree whereas the training set keeps getting lower and lower, creating a huge gap between the testing and training error curves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Look at the learning curves for the decision tree regressor with max depth 1 and 10 (first and last learning curve graphs). When the model is fully trained does it suffer from either high bias/underfitting or high variance/overfitting?***\n",
    "\n",
    "As the depth increases, the variance is getting higher. An overfitting problem starts to appear where the training error is close to zero but the testing error stays the same. That indicates an inability to generalize well to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Look at the model complexity graph. How do the training and test error relate to increasing model complexity? ***\n",
    "\n",
    "<img src=\"model complexity.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we saw with the previous graphics, are the complexity of the model inceases, the gap between the testing and training error grows wider. Testing error stays approximatly the same after a certain point. So there is no need to increase the complexity beyond that point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Based on this relationship, which model (max depth) best generalizes the dataset and why?***\n",
    "\n",
    "A depth of 4 or 5 is good enough for generalization. Beyond that the training error is getting lower, but not the testing error. If increasing the complexity of the model does not allow better generalization, then we should stick to a point where testing and training error are closer. We will save computation time as a side effect."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4) Model Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Model makes predicted housing price with detailed model parameters***\n",
    "\n",
    "GridSearchCV(cv=None, error_score='raise',\n",
    "       estimator=DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
    "           max_leaf_nodes=None, min_samples_leaf=1, min_samples_split=2,\n",
    "           min_weight_fraction_leaf=0.0, random_state=None,\n",
    "           splitter='best'),\n",
    "       fit_params={}, iid=True, loss_func=None, n_jobs=1,\n",
    "       param_grid={'max_depth': (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)},\n",
    "       pre_dispatch='2*n_jobs', refit=True, score_func=None,\n",
    "       scoring=make_scorer(mean_absolute_error), verbose=0)\n",
    "       \n",
    "       \n",
    "House: [11.95, 0.0, 18.1, 0, 0.659, 5.609, 90.0, 1.385, 24, 680.0, 20.2, 332.09, 12.13]\n",
    "\n",
    "Prediction: [ 19.93372093]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Compare prediction to earlier statistics***\n",
    "\n",
    "The predicted price is 19,933.72$. It is inside one standard deviation from the mean and not too far from the mean and the median.\n",
    "\n",
    "The interquartile range of the provided examples is 17,025-25,000, so the predicted price is well between the outliers range.\n",
    "\n",
    "Minimum house price                : 5,000.00<br>\n",
    "Maximum house price                : 50,000.00<br>\n",
    "Mean house price                   : 22,532.81<br>\n",
    "Median house price                 : 21,200.00<br>\n",
    "Standard deviation of house prices : 9,188.01<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
